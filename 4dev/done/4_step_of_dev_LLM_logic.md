# Шаг 4: Основная Логика LLM и RAG-Цепочки

**Цель:** Создать центральный логический компонент — RAG-цепочку, которая будет принимать запрос пользователя, обогащать его контекстом из базы знаний и истории диалога, и генерировать осмысленный ответ с помощью LLM. Вся логика будет инкапсулирована в новом файле `app/core/chain.py`.

---

### **Этап 1: Создание Системного Промпта и Шаблона**

На этом этапе мы определим "личность" и инструкции для AI-ассистента. Мы создадим шаблон промпта, который будет направлять модель, как использовать предоставленный контекст и историю чата для генерации ответов.

#### **1.1. Определение Системного Промпта**
*   **Цель:** Сформулировать детальные инструкции для LLM, определяющие ее роль, стиль общения и правила использования контекста.
*   **Файл:** `app/core/chain.py` (новый файл).
*   **Логика:**
    *   Создать многострочную строковую константу `SYSTEM_PROMPT`.
    *   В промпте указать:
        *   **Роль:** "Ты — Александр Самохвалов, бэкенд-разработчик на Python. Твоя задача — профессионально и дружелюбно отвечать на вопросы о твоем опыте, проектах и технических навыках."
        *   **Источник знаний:** "Используй предоставленный 'Контекст из базы знаний' как основной источник информации для ответов."
        *   **История диалога:** "Учитывай 'Историю чата', чтобы твои ответы были последовательными и релевантными в рамках текущего диалога."
        *   **Правила поведения:** "Если в контексте нет ответа, вежливо сообщи, что у тебя нет информации по этому вопросу. Не придумывай факты. Общайся на 'Вы'."

#### **1.2. Создание Шаблона Промпта**
*   **Цель:** Создать структуру для финального промпта, который будет отправлен в LLM.
*   **Файл:** `app/core/chain.py`.
*   **Логика:**
    *   Импортировать `ChatPromptTemplate` и `MessagesPlaceholder` из `langchain_core.prompts`.
    *   Создать экземпляр `ChatPromptTemplate.from_messages`, который будет включать:
        1.  `("system", SYSTEM_PROMPT)` — системное сообщение с инструкциями.
        2.  `MessagesPlaceholder(variable_name="chat_history")` — плейсхолдер для истории сообщений.
        3.  `("human", ...)` — шаблон для вопроса пользователя, включающий сам вопрос (`{question}`) и извлеченный контекст (`{context}`).

---

### **Этап 2: Реализация Функции-Фабрики для RAG-цепочки**

На этом этапе мы создадим основную функцию, которая конструирует и возвращает исполняемую RAG-цепочку с помощью LangChain Expression Language (LCEL).

#### **2.1. Создание Фабрики Цепочек**
*   **Цель:** Создать основную функцию, которая конструирует и возвращает исполняемую RAG-цепочку.
*   **Файл:** `app/core/chain.py`.
*   **Функция / Сигнатура:** `create_rag_chain() -> Runnable`.
*   **Логика:**
    1.  **Инициализация компонентов:** Внутри функции будут инициализированы `ChatMistralAI` (наша LLM) и ретривер из `ChromaDB` (с помощью функции `get_vector_store` из `app/core/rag.py`).
    2.  **Построение цепочки (LCEL):** Будет создана цепочка, выполняющая следующие шаги:
        *   Принятие на вход вопроса пользователя (`question`) и истории чата (`chat_history`).
        *   Передача вопроса пользователя ретриверу для извлечения релевантных документов (контекста).
        *   Формирование словаря со всеми необходимыми данными (`context`, `question`, `chat_history`).
        *   Передача этого словаря в `ChatPromptTemplate` (из Этапа 1) для создания финального промпта.
        *   Передача промпта в модель `ChatMistralAI` для генерации ответа.
        *   Парсинг ответа в строковый формат с помощью `StrOutputParser`.
    3.  **Возврат:** Функция вернет готовую к использованию, исполняемую цепочку (`Runnable`).